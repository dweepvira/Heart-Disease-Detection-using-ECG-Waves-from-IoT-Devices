{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-05T14:47:34.700931Z","iopub.status.busy":"2024-04-05T14:47:34.700555Z","iopub.status.idle":"2024-04-05T14:47:34.710107Z","shell.execute_reply":"2024-04-05T14:47:34.708973Z","shell.execute_reply.started":"2024-04-05T14:47:34.700905Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T14:47:55.710369Z","iopub.status.busy":"2024-04-05T14:47:55.709962Z","iopub.status.idle":"2024-04-05T15:02:25.539689Z","shell.execute_reply":"2024-04-05T15:02:25.538816Z","shell.execute_reply.started":"2024-04-05T14:47:55.710341Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense ,Dropout , BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","#importing libraries\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from seaborn import heatmap\n","from tensorflow import keras\n","from tensorflow.keras.layers import Conv1D,MaxPool1D,BatchNormalization,Bidirectional,LSTM,Flatten,Input,Dense\n","from keras import Model\n","from keras.callbacks import EarlyStopping,ModelCheckpoint\n","from imblearn.over_sampling import SMOTE\n","from sklearn.metrics import classification_report , confusion_matrix\n","from tensorflow.keras.utils import to_categorical\n","import warnings\n","\n","# Load training and testing CSV files\n","\n","\n","#Imbalanced found so rectifying the imbalance\n","smote = SMOTE (sampling_strategy='all',random_state=42)\n","x =train_df.drop (columns =187)\n","y = train_df[187]\n","x,y = smote.fit_resample (x,y)\n","train_df = pd.concat([x,y],axis = 1)\n","# randomize dataset to prevent bias if any \n","train_df = train_df.sample (frac =1) \n","\n","X_train = train_df.iloc[:, :-1].values\n","y_train = train_df.iloc[:, -1].values\n","X_test = test_df.iloc[:, :-1].values\n","y_test = test_df.iloc[:, -1].values\n","\n","# Integer encode the target labels\n","label_encoder = LabelEncoder()\n","y_train = label_encoder.fit_transform(y_train)\n","y_test = label_encoder.transform(y_test)\n","\n","# Reshape input data for 1D convolutional layer\n","X_train = np.expand_dims(X_train, axis=-1)\n","X_test = np.expand_dims(X_test, axis=-1)\n","\n","# Build TCN model\n","num_classes = len(np.unique(y_train))\n","model = Sequential([\n","    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=X_train.shape[1:]),\n","    BatchNormalization(),\n","    MaxPooling1D(pool_size=2),\n","    Dropout(0.5),\n","    Conv1D(filters=128, kernel_size=5, activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling1D(pool_size=2),\n","    Dropout(0.5),\n","    Flatten(),\n","    Dense(100, activation='relu'),\n","    Dense(num_classes, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n","\n","# Define early stopping callback based on validation loss\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","# Fit the model\n","history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n","\n","# Evaluate the model on test data\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","\n","print(f'Test Loss: {test_loss}')\n","print(f'Test Accuracy: {test_accuracy}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:04:04.045894Z","iopub.status.busy":"2024-04-05T15:04:04.045490Z","iopub.status.idle":"2024-04-05T15:04:04.290532Z","shell.execute_reply":"2024-04-05T15:04:04.289613Z","shell.execute_reply.started":"2024-04-05T15:04:04.045862Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n","\n","# Plot training history\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:05:21.582954Z","iopub.status.busy":"2024-04-05T15:05:21.582580Z","iopub.status.idle":"2024-04-05T15:05:21.827595Z","shell.execute_reply":"2024-04-05T15:05:21.826715Z","shell.execute_reply.started":"2024-04-05T15:05:21.582924Z"},"trusted":true},"outputs":[],"source":["plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:06:52.704009Z","iopub.status.busy":"2024-04-05T15:06:52.702815Z","iopub.status.idle":"2024-04-05T15:06:54.343105Z","shell.execute_reply":"2024-04-05T15:06:54.342089Z","shell.execute_reply.started":"2024-04-05T15:06:52.703975Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","# Convert integer labels to one-hot encoded labels\n","y_test_onehot = to_categorical(y_test)\n","\n","# Get true and predicted class labels from model predictions\n","y_true_classes = np.argmax(y_test_onehot, axis=1)  # Convert one-hot encoded labels to class labels\n","y_pred_classes = np.argmax(model.predict(X_test), axis=1)\n","\n","# Compute confusion matrix\n","conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=np.unique(y_true_classes), yticklabels=np.unique(y_true_classes))\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T15:18:09.803472Z","iopub.status.busy":"2024-04-05T15:18:09.802598Z","iopub.status.idle":"2024-04-05T15:18:09.853551Z","shell.execute_reply":"2024-04-05T15:18:09.852495Z","shell.execute_reply.started":"2024-04-05T15:18:09.803437Z"},"trusted":true},"outputs":[],"source":["# Save the model\n","model.save(\"tcn_model.h5\")\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T16:57:22.104970Z","iopub.status.busy":"2024-04-05T16:57:22.104263Z","iopub.status.idle":"2024-04-05T16:57:26.867793Z","shell.execute_reply":"2024-04-05T16:57:26.866707Z","shell.execute_reply.started":"2024-04-05T16:57:22.104939Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Index of one data point for each class:\n","{0.0: 0, 1.0: 72470, 2.0: 74693, 3.0: 80481, 4.0: 81122}\n"]}],"source":["import pandas as pd\n","\n","# Load the CSV dataset into a DataFrame\n","df = pd.read_csv(\"/kaggle/input/heartbeat/mitbih_train.csv\")\n","\n","# Get the unique class labels present in the dataset\n","unique_classes = df.iloc[:, -1].unique()\n","\n","# Dictionary to store the index of one data point for each class\n","one_data_point_index_per_class = {}\n","\n","# Loop through each unique class label\n","for class_label in unique_classes:\n","    # Find the index of the first occurrence of the class label\n","    data_point_index = df[df.iloc[:, -1] == class_label].index[0]\n","    # Store the index of the selected data point for this class label\n","    one_data_point_index_per_class[class_label] = data_point_index\n","\n","# Print the dictionary containing the index of one data point for each class\n","print(\"Index of one data point for each class:\")\n","print(one_data_point_index_per_class)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-05T17:06:54.211411Z","iopub.status.busy":"2024-04-05T17:06:54.210515Z","iopub.status.idle":"2024-04-05T17:06:54.265285Z","shell.execute_reply":"2024-04-05T17:06:54.264100Z","shell.execute_reply.started":"2024-04-05T17:06:54.211376Z"},"trusted":true},"outputs":[{"ename":"TypeError","evalue":"Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 187, 1], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/College/Major Project/tcn_model.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmitbih_train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    231\u001b[0m         filepath,\n\u001b[0;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:870\u001b[0m, in \u001b[0;36mLayer.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    873\u001b[0m     )\n","\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'InputLayer' using config={'batch_shape': [None, 187, 1], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer'}.\n\nException encountered: Unrecognized keyword arguments: ['batch_shape']"]}],"source":["from tensorflow.keras.models import load_model\n","import numpy as np\n","import pandas as pd\n","\n","# Load the saved model\n","loaded_model = load_model(\"D:/College/Major Project/tcn_model.h5\")\n","train_df = pd.read_csv(\"mitbih_train.csv\", header=None)\n","X_train = train_df.iloc[:, :-1].values\n","y_train = train_df.iloc[:, -1].values\n","\n","# Example: Selecting one data point from the dataset\n","data_point_index = 0  # Adjust this index to select the desired data point\n","data_point = X_train[0]  # Assuming X_test is your dataset\n","data_point = np.expand_dims(data_point, axis=0)  # Add batch dimension\n","\n","# Make prediction on the data point\n","prediction = loaded_model.predict(data_point)\n","\n","# Print the predicted class probabilities\n","print(\"Predicted probabilities:\", prediction)\n","\n","# Get the predicted class (assuming it's the class with highest probability)\n","predicted_class = np.argmax(prediction)\n","print(\"Predicted class:\", predicted_class)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":21941,"sourceId":26055,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
